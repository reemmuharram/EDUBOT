# ğŸ“š Student Assistant Chatbot

 Intelligent RAG chatbot designed to help students with their questions. Built with FastAPI, Streamlit, and powered by Google's Flan-T5 model.

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)

## ğŸš€ Streamlit Demo
[You can try it here ğŸ¤–](https://edubot-ebc2frxtf7jw2su44si2ks.streamlit.app/)



## âœ¨ Features

- ğŸ¤– **AI-Powered Responses**: Uses Google Flan-T5 for natural language understanding
- ğŸ” **Smart Retrieval**: FAISS vector search finds the most relevant information
- ğŸ“Š **22,000+ Q&A Pairs**: Comprehensive educational dataset
- âš¡ **Fast & Efficient**: Optimized for CPU inference
---

## ğŸ¬ Quick Start

### Prerequisites

- Python 3.11 or higher
- 4GB+ RAM recommended

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/chatbot-assistant.git
   cd chatbot-assistant
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv rag_env
   
   # On Windows:
   rag_env\Scripts\activate
   
   # On Mac/Linux:
   source rag_env/bin/activate
   ```

3. **Install PyTorch (CPU version)**
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
   ```

4. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

5. **Build the vectorstore** (first time only)
   ```bash
   python rebuild_vectorstore.py
   ```
   *This will take 5-10 minutes. It processes your dataset and creates searchable embeddings.*

---

## ğŸš€ Running the Application

### Option 1: Streamlit Interface

```bash
streamlit run streamlit_app/app.py
```

Open your browser at: **http://localhost:8501**

### Option 2: FastAPI Web Interface

```bash
python main.py
```

Open your browser at: **http://localhost:8000**

---

## ğŸ“ Project Structure

```
chatbot-assistant/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ full_dataset.csv          # Your Q&A training data (22,571 pairs)
â”‚
â”œâ”€â”€ model/                         # Downloaded Flan-T5 model (auto-created)
â”‚
â”œâ”€â”€ vectorstore/                   # FAISS vector database
â”‚   â”œâ”€â”€ index.faiss               # Vector indices
â”‚   â””â”€â”€ index.pkl                 # Metadata
â”‚
â”œâ”€â”€ rag/                          # RAG pipeline modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embeddings.py             # Sentence transformers setup
â”‚   â”œâ”€â”€ vectorstore.py            # FAISS operations
â”‚   â”œâ”€â”€ retriever.py              # Search logic
â”‚   â”œâ”€â”€ prompts.py                # Prompt templates
â”‚   â”œâ”€â”€ chain.py                  # LangChain integration
â”‚   â”œâ”€â”€ loader.py                 # Data loading utilities
â”‚   â””â”€â”€ splitter.py               # Text chunking
â”‚
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py                    # Streamlit chat interface
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                # FastAPI web UI
â”‚
â”œâ”€â”€ main.py                       # FastAPI application
â”œâ”€â”€ rebuild_vectorstore.py        # Vectorstore builder script
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # You are here! ğŸ‘‹
```

---

## ğŸ› ï¸ How It Works

### The RAG Pipeline

1. **User Question** â†’ You ask: "How do I apply for admission?"

2. **Embedding** â†’ Your question is converted to a vector (numerical representation)

3. **Retrieval** â†’ FAISS searches the vectorstore for the 3 most similar Q&A pairs

4. **Context Building** â†’ Retrieved information is formatted as context

5. **Generation** â†’ Flan-T5 generates a friendly, natural answer based on the context

6. **Response** â†’ You get: "You can apply for admission by filling out the application form."

### Tech Stack

- **LangChain**: Orchestrates the RAG pipeline
- **FAISS**: Lightning-fast vector similarity search
- **Sentence Transformers**: Creates semantic embeddings
- **Google Flan-T5**: Generates natural language responses
- **FastAPI**: Modern, fast web framework
- **Streamlit**: Interactive data apps

---
### Update the Dataset

1. Replace `dataset/full_dataset.csv` with your data
2. Ensure columns are named: `question` and `answer` (or `input` and `target`)
3. Run: `python rebuild_vectorstore.py`

### Adjust AI Parameters

In `rag/chain.py`, modify the LLM settings:

```python
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=150,  # Longer responses
    temperature=0.8,     # More creative (0.0 = deterministic, 1.0 = creative)
)
```

---

## ğŸ› Troubleshooting

### "Module not found" errors
```bash
pip install -r requirements.txt
```

### "Vectorstore not found"
```bash
python rebuild_vectorstore.py
```
### Slow model downloads
Models download on first run (~900MB). Be patient or use faster internet.

### Out of memory
Reduce `k=3` to `k=2` in `rag/retriever.py` for less context

---

## ğŸ“Š Performance

- **Response Time**: 2-5 seconds per query
- **Memory Usage**: ~2GB RAM
- **Dataset Size**: 22,571 Q&A pairs
- **Model Size**: ~900MB (Flan-T5-base)
- **Embedding Model**: ~90MB (all-MiniLM-L6-v2)
## â­ Star History

If you find this project helpful, please consider giving it a star! â­

<div align="center">
  Made with â˜• 
</div>
